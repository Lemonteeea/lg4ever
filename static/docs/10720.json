{"title":"19 | 逻辑回归：如何让计算机做出二值化决策？","context":"\n                    <p data-nodeid=\"48070\" class=\"\">\n                      在上一讲，学习完 AI\n                      的基本框架后，我们现在就开始围绕当前人工智能领域最常用的模型，来分别学习一下它们背后的原理。\n                    </p>\n                    <p data-nodeid=\"48071\">\n                      这一讲，我们从最常见的逻辑回归模型说起，逻辑回归是人工智能领域中入门级的基础模型，它在很多领域都有应用，例如用户的信贷模型、疾病识别等。\n                    </p>\n                    <p data-nodeid=\"48072\">\n                      逻辑回归是一种分类模型，可以对一个输入\n                      x，识别并预测出一个二值化的类别标签\n                      y。例如，要预测照片中人物的性别，可以采用逻辑回归建立模型。给模型输入一个描述照片的特征向量\n                      x，经过模型的计算，可以得到输出值 y 为“男”或“女”。\n                    </p>\n                    <p data-nodeid=\"48073\">\n                      在深入学习逻辑回归的原理之前，我们先来了解一下什么是分类问题，以及分类问题有哪些类型。\n                    </p>\n                    <h3 data-nodeid=\"48074\">分类问题</h3>\n                    <p data-nodeid=\"48075\">\n                      在人工智能领域中，分类问题是特别常见的一种问题类型。简而言之，分类问题就是对一个测试验本去预测它归属的类别。例如，预测胎儿性别、预测足球比赛结果。\n                    </p>\n                    <p data-nodeid=\"48076\">\n                      根据归属类别可能性的数量，分类问题又可以分为二分类问题和多分类问题。\n                    </p>\n                    <ul data-nodeid=\"48077\">\n                      <li data-nodeid=\"48078\">\n                        <p data-nodeid=\"48079\">\n                          二分类问题，顾名思义就是预测的归属类别只有两个。例如，预测性别男/女、预测主场球队的胜负、预测明天是否下雨。\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48080\">\n                        <p data-nodeid=\"48081\">\n                          多分类问题，预测的归属类别大于两个的那类问题。例如，预测足球比赛结果是胜、负，还是平局；预测明天天气是雨天、晴天，还是阴天。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"48082\">\n                      在研究分类的建模算法时，人们往往会从二分类问题入手，这主要是因为多分类问题可以用多个二分类问题来表示。例如，预测明天天气是雨天、晴天，还是阴天，这是个多分类问题（三分类）；它也可以表示为，预测明天是否下雨、预测明天是否晴天、预测明天是否阴天，这三个二分类问题。\n                    </p>\n                    <p data-nodeid=\"48083\">\n                      因此，二分类问题是分类问题的基础，在讨论分类算法时，人们往往会从二分类问题入手。\n                    </p>\n                    <h3 data-nodeid=\"48084\">逻辑回归及其建模流程</h3>\n                    <p data-nodeid=\"48085\">\n                      逻辑回归（Logistic\n                      Regression，LR）是人工智能领域非常经典的算法之一，它可以用来对二分类问题进行建模，对于一个给定的输入，可以预测其类别为正\n                      1 或负 0。接下来，我们就从 AI 基本框架的 3\n                      个公式，来学习一下 LR 的建模流程。\n                    </p>\n                    <p data-nodeid=\"48086\">\n                      重温一下人工智能基本框架的 3 个公式分别是：\n                    </p>\n                    <ul data-nodeid=\"48087\">\n                      <li data-nodeid=\"48088\">\n                        <p data-nodeid=\"48089\">\n                          第一步，根据假设，写出模型的输入、输出关系 y = f(<i\n                            ><strong data-nodeid=\"48222\">w</strong></i\n                          >; x)；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48090\">\n                        <p data-nodeid=\"48091\">\n                          第二步，根据偏差的计算方法，写出描述偏差的损失函数\n                          L(<i><strong data-nodeid=\"48230\">w</strong></i\n                          >)；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48092\">\n                        <p data-nodeid=\"48093\">\n                          第三步，对于损失函数，求解最优的参数值，即\n                          <i><strong data-nodeid=\"48245\">w</strong></i\n                          >*= argmin L(<i\n                            ><strong data-nodeid=\"48246\">w</strong></i\n                          >)。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"48094\">\n                      接下来，我会逐一展示这三步的过程。\n                    </p>\n                    <h4 data-nodeid=\"48095\">\n                      1.模型的输入、输出关系（Sigmoid 函数）\n                    </h4>\n                    <p data-nodeid=\"48096\">\n                      在逻辑回归中，第一个公式的表达式非常简单，为 y=f(<i\n                        ><strong data-nodeid=\"48271\">w;x</strong></i\n                      >)=sigmoid(<i><strong data-nodeid=\"48272\">w·x</strong></i\n                      >)=1/(1+e<sup\n                        >-<i><strong data-nodeid=\"48273\">w·x</strong></i></sup\n                      >)。\n                    </p>\n                    <p data-nodeid=\"48097\">\n                      直观上来看，逻辑回归的模型假设是，把模型参数向量\n                      <i><strong data-nodeid=\"48287\">w</strong></i> 和输入向量\n                      <i><strong data-nodeid=\"48288\">x</strong></i>\n                      的点乘（即线性变换）结果输入给 Sigmoid\n                      函数中，即可得到预测值 y。\n                    </p>\n                    <p data-nodeid=\"48098\">\n                      此时的预测值 y 还是个 0～1 之间的连续值，这是因为 Sigmoid\n                      函数的值域是\n                      (0,1)。逻辑回归是个二分类模型，它的最终输出值只能是两个类别标签之一。通常，我们习惯于用“0”和“1”来分别标记二分类的两个类别。\n                    </p>\n                    <p data-nodeid=\"48099\">\n                      在逻辑回归中，常用预测值 y 和 0.5\n                      的大小关系，来判断样本的类别归属。<strong\n                        data-nodeid=\"48294\"\n                        >具体地，预测值 y 如果大于 0.5，则认为预测的类别为\n                        1；反之，则预测的类别为 0。</strong\n                      >\n                    </p>\n                    <p data-nodeid=\"48100\">\n                      我们把上面的描述进行总结，来汇总一下逻辑回归输入向量、预测值和类别标签之间的关系，则有下面的流程图。\n                    </p>\n                    <p data-nodeid=\"48101\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lxJuAJjk2AAE7vKJA3Cg656.png\"\n                        alt=\"图片1.png\"\n                        data-nodeid=\"48298\"\n                      />\n                    </p>\n                    <p data-nodeid=\"48102\">\n                      为了深入了解逻辑回归的模型假设，我们需要先认识下 Sigmoid\n                      函数。Sigmoid 函数的表达式为 y =\n                      sigmoid(x)=1/(1+e<sup>-x</sup>)，它是个单调递增函数，定义域为\n                      (-∞, +∞)，值域为 (0,1)，它的函数图像如下。<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lxKuAaTt2AAEuW5MrG8c228.png\"\n                        alt=\"图片2.png\"\n                        data-nodeid=\"48307\"\n                      /><br />\n                      我们可以看出，Sigmoid 函数可以将任意一个实数\n                      x，单调地映射到 0 到 1\n                      的区间内，这正好符合了“概率”的取值范围。\n                    </p>\n                    <p data-nodeid=\"48103\">\n                      我们还可以用求导公式来看一下 Sigmoid 函数的一阶导数。<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/04/15/Cip5yF_pVFqAQwAPAAA8cpQoHKA089.png\"\n                        alt=\"WechatIMG1422.png\"\n                        data-nodeid=\"48314\"\n                      />\n                    </p>\n                    <h4 data-nodeid=\"48104\">2.逻辑回归的损失函数</h4>\n                    <p data-nodeid=\"48105\">\n                      有了这些基本假设后，我们尝试根据偏差的计算方法，写出描述偏差的损失函数\n                      L(<i><strong data-nodeid=\"48323\">w</strong></i\n                      >)。\n                    </p>\n                    <p data-nodeid=\"48106\">\n                      我们刚刚提到过，逻辑回归预测结果的值域 y 为\n                      (0,1)，代表的是样本属于类别 1 的概率。\n                    </p>\n                    <ul data-nodeid=\"48107\">\n                      <li data-nodeid=\"48108\">\n                        <p data-nodeid=\"48109\">\n                          具体而言，如果样本属于类别“1”的概率大于\n                          0.5，则认为样本的预测类别为“1”；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48110\">\n                        <p data-nodeid=\"48111\">\n                          如果样本属于类别“1”的概率小于\n                          0.5，则认为样本的预测类别为“0”。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"48112\">\n                      这里出现了这么多的概率，我们可以借鉴在《09 |\n                      似然估计：如何利用 MLE\n                      对参数进行估计？》中学的概率计算和极大似然估计的思想，尝试写出样本被正确预测的概率。<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/3D/CgqCHl_pVHaAOfpZAAC78iA7-nA342.png\"\n                        alt=\"WechatIMG1421.png\"\n                        data-nodeid=\"48333\"\n                      /><br />\n                      我们将上面两个等式合并，就可以得到某个数据<i\n                        ><strong data-nodeid=\"48379\">x</strong></i\n                      ><sub>i</sub> 被正确预测的概率，即\n                      P(y<sub>i</sub>|x<sub>i</sub>,w)=Φ(z<sub>i</sub>)<sup\n                        >y<sub>i</sub></sup\n                      >·[1-Φ(z<sub>i</sub>)]<sup>1-y<sub>i</sub></sup\n                      >。\n                    </p>\n                    <ul data-nodeid=\"48113\">\n                      <li data-nodeid=\"48114\">\n                        <p data-nodeid=\"48115\">\n                          如果真实结果 y<sub>i</sub> 为 1，则\n                          P(y<sub>i</sub>|x<sub>i</sub>,w) =\n                          Φ(z<sub>i</sub>)，描述的是样本被预测为类别“1”的概率；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48116\">\n                        <p data-nodeid=\"48117\">\n                          如果真实结果 y<sub>i</sub> 为 0，则\n                          P(y<sub>i</sub>|x<sub>i</sub>,w) =\n                          1-Φ(z<sub>i</sub>)，描述的是样本被预测为类别“0”的概率。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"48118\">\n                      接下来可以将上式扩展到整个样本数据集中，则可采用极大似然估计得到\n                      L(<i><strong data-nodeid=\"48427\">w</strong></i\n                      >)，即<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/19/Ciqc1F_lxSyACUiYAAA4FTEM9qs111.png\"\n                        alt=\"图片4.png\"\n                        data-nodeid=\"48426\"\n                      />\n                    </p>\n                    <p data-nodeid=\"48119\">\n                      我们之前在《09 | 似然估计：如何利用 MLE\n                      对参数进行估计？》学习极大似然估计 MLE\n                      时，曾经提过一个常用的公式化简方法，那就是通过取对数，让连续乘积的大型运算变为连续求和，则有<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/03/FE/CgpVE1_lxTyASfq6AAA1M4H_6RI019.png\"\n                        alt=\"图片5.png\"\n                        data-nodeid=\"48434\"\n                      />\n                    </p>\n                    <h4 data-nodeid=\"48120\">\n                      <strong data-nodeid=\"48438\"\n                        >3.求解最优的模型参数值</strong\n                      >\n                    </h4>\n                    <p data-nodeid=\"48121\">\n                      AI\n                      建模框架的最后一步，就是对损失函数求解最优的参数值，即<i\n                        ><strong data-nodeid=\"48497\">w</strong></i\n                      >*= argmin l(<i><strong data-nodeid=\"48498\">w</strong></i\n                      >)。刚刚我们求得，损失函数为<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/03/FE/CgpVE1_lxUmAc_dEAAA5bSwYYTY904.png\"\n                        alt=\"图片6.png\"\n                        data-nodeid=\"48456\"\n                      /><br />\n                      可见，损失函数是个关于\n                      <i><strong data-nodeid=\"48499\">x</strong></i\n                      ><sub>i</sub>、y<sub>i</sub> 和\n                      <i><strong data-nodeid=\"48500\">w</strong></i> 的函数，而<i\n                        ><strong data-nodeid=\"48501\">x</strong></i\n                      ><sub>i</sub> 和 y<sub>i</sub>\n                      是输入数据集中已知的条件，所以损失函数的未知数只有\n                      <i><strong data-nodeid=\"48502\">w</strong></i\n                      >。\n                    </p>\n                    <p data-nodeid=\"48122\">\n                      于是可以得到结论，逻辑回归最后一步的建模公式，实质上就是求解函数极值的问题。\n                    </p>\n                    <blockquote data-nodeid=\"48123\">\n                      <p data-nodeid=\"48124\">\n                        关于求极值，我们在《05 |\n                        求极值：如何找到复杂业务的最优解？》曾详细介绍过求导法和梯度下降法。\n                      </p>\n                    </blockquote>\n                    <p data-nodeid=\"48125\">\n                      在这里，由于损失函数包含了非线性的 sigmoid\n                      函数，求导法是无法得到解析解的；因此，我们使用梯度下降法来求解参数<i\n                        ><strong data-nodeid=\"48524\">w</strong></i\n                      >。<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lxYeAZOMOAADJ2CJBEHc898.png\"\n                        alt=\"图片7.png\"\n                        data-nodeid=\"48517\"\n                      /><br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lxb2ALAYhAACvGE2H5Rw210.png\"\n                        alt=\"图片8.png\"\n                        data-nodeid=\"48521\"\n                      /><br />\n                      我们已经计算出了损失函数关于模型参数的导数，这也是损失函数的梯度方向，我们可以利用先前所学的梯度下降法来求解函数的极值。\n                    </p>\n                    <p data-nodeid=\"48126\">\n                      然而，这里存在一个计算效率的缺陷，即梯度函数中包含了大型求和的运算。这里的大型求和是\n                      i 从 1 到 n\n                      的计算，也就是对于整个数据集全部的数据去进行的全量计算。\n                    </p>\n                    <blockquote data-nodeid=\"48127\">\n                      <p data-nodeid=\"48128\">\n                        可以想象出，当输入的数据量非常大的时候，梯度下降法每次的迭代都会产生大量的计算。这样，建模过程中会消耗大量计算资源，模型更新效率也会受到很大影响。\n                      </p>\n                    </blockquote>\n                    <h4 data-nodeid=\"48129\">【随机梯度下降法】</h4>\n                    <p data-nodeid=\"48130\">\n                      为了解决这个问题，人工智能领域常常用<strong\n                        data-nodeid=\"48541\"\n                        >随机梯度下降法</strong\n                      >来修正<strong data-nodeid=\"48542\">梯度下降法</strong\n                      >的不足。随机梯度下降法与梯度下降法的区别只有一点，那就是随机梯度下降在每轮更新参数时，只随机选取一个样本\n                      d<sub>m</sub>\n                      来计算梯度，而非计算整个数据集梯度。其余的计算过程，二者完全一致。\n                    </p>\n                    <p data-nodeid=\"48131\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lxdaAQ749AADJ2O9gnkU384.png\"\n                        alt=\"图片9.png\"\n                        data-nodeid=\"48545\"\n                      />\n                    </p>\n                    <p data-nodeid=\"48132\">\n                      根据上面更新公式的算法，我们通过多轮迭代，就能最终求解出让\n                      l(w) 取得最大值的参数向量<i\n                        ><strong data-nodeid=\"48553\">w</strong></i\n                      >。\n                    </p>\n                    <h3 data-nodeid=\"48133\">逻辑回归代码实现</h3>\n                    <p data-nodeid=\"48134\">\n                      接下来，我们在下面的数据集上，分别采用逻辑回归来建立分类模型。\n                    </p>\n                    <p data-nodeid=\"48135\">\n                      第一个数据集如下，其中每一行是一个样本，每一列一个特征，最后一列是样本的类别。\n                    </p>\n                    <p data-nodeid=\"48136\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lxgCAcT3PAADKvRlRWh8558.png\"\n                        alt=\"图片10.png\"\n                        data-nodeid=\"48559\"\n                      />\n                    </p>\n                    <p data-nodeid=\"48137\">\n                      第二个数据集如下，格式与第一个数据集相同。\n                    </p>\n                    <p data-nodeid=\"48138\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lxgaAbVaDAADgP88xevs672.png\"\n                        alt=\"图片11.png\"\n                        data-nodeid=\"48563\"\n                      />\n                    </p>\n                    <p data-nodeid=\"48139\">\n                      我们采用下面的代码，建立逻辑回归模型。\n                    </p>\n                    <div class=\"course-code-area\">\n                      <div class=\"copy-btn\">\n                        <div class=\"copy-icon\"></div>\n                        复制代码\n                      </div>\n                      <pre><code data-language=\"python\"><ol><li><div class=\"code-word\"><span class=\"hljs-keyword\">import</span> math\n</div></li><li><div class=\"code-word\"><span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n</div></li><li><div class=\"code-word\"><span class=\"hljs-keyword\">import</span> random\n</div></li><li><div class=\"code-word\">x = np.array([[<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>],[<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">1</span>],[<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>],[<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">1</span>]])\n</div></li><li><div class=\"code-word\">y = np.array([<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>])\n</div></li><li><div class=\"code-word\"><span class=\"hljs-comment\">#y = np.array([0,0,1,1])</span>\n</div></li><li><div class=\"code-word\">w = np.array([<span class=\"hljs-number\">0.5</span>,<span class=\"hljs-number\">0.5</span>,<span class=\"hljs-number\">0.5</span>])\n</div></li><li><div class=\"code-word\">a = <span class=\"hljs-number\">0.01</span>\n</div></li><li><div class=\"code-word\">maxloop = <span class=\"hljs-number\">10000</span>\n</div></li><li><div class=\"code-word\"><span class=\"hljs-keyword\">for</span> _ <span class=\"hljs-keyword\">in</span> range(maxloop):\n</div></li><li><div class=\"code-word\">\tm = random.randint(<span class=\"hljs-number\">0</span>,<span class=\"hljs-number\">3</span>)\n</div></li><li><div class=\"code-word\">\tfi = <span class=\"hljs-number\">1.0</span>/(<span class=\"hljs-number\">1</span>+math.pow(math.e,-np.dot(x[m],w)))\n</div></li><li><div class=\"code-word\">\tg = (y[m] - fi)*x[m]\n</div></li><li><div class=\"code-word\">\tw = w + a*g\n</div></li><li><div class=\"code-word\">\t<span class=\"hljs-keyword\">print</span> w\n</div></li></ol></code></pre>\n                    </div>\n                    <p data-nodeid=\"48141\">【我们对代码进行走读】</p>\n                    <ul data-nodeid=\"48142\">\n                      <li data-nodeid=\"48143\">\n                        <p data-nodeid=\"48144\">\n                          代码中，第 5～7 行分别输入数据集 x 和 y；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48145\">\n                        <p data-nodeid=\"48146\">\n                          第 9\n                          行，初始化参数向量，在这里，我们采用固定的初始化方法，你也可以调整为随机初始化；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48147\">\n                        <p data-nodeid=\"48148\">第 11 行，设置学习率为 0.01；</p>\n                      </li>\n                      <li data-nodeid=\"48149\">\n                        <p data-nodeid=\"48150\">\n                          第 12 行，设置最大迭代轮数为 10000 次。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"48151\">\n                      接下来进入随机梯度下降法的循环体。\n                    </p>\n                    <ul data-nodeid=\"48152\">\n                      <li data-nodeid=\"48153\">\n                        <p data-nodeid=\"48154\">\n                          第 14 行，从 0 到 3\n                          中随机抽取一个数字作为本轮迭代梯度的样本；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48155\">\n                        <p data-nodeid=\"48156\">\n                          第 15 行，计算 Φ(z<sub>m</sub>)；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48157\">\n                        <p data-nodeid=\"48158\">\n                          第 16 行，计算样本 m 带来的梯度 g；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48159\">\n                        <p data-nodeid=\"48160\">\n                          第 17 行，利用随机梯度下降法更新参数 w；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48161\">\n                        <p data-nodeid=\"48162\">第 18 行，打印这一轮的结果。</p>\n                      </li>\n                    </ul>\n                    <h4 data-nodeid=\"48163\">【数据集一】</h4>\n                    <p data-nodeid=\"48164\">\n                      运行上述代码，我们对数据集一建模得到的最优参数为\n                      [3.1,3.0,-4.8]。利用这组参数，我们可以对数据集一的学习效果进行测试，如下表所示\n                    </p>\n                    <p data-nodeid=\"48165\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lxhOAFnR4AAIK9izy6Cs182.png\"\n                        alt=\"图片12.png\"\n                        data-nodeid=\"48588\"\n                      />\n                    </p>\n                    <p data-nodeid=\"48166\">\n                      可见，数据集一上，我们的模型全部正确预测，效果非常好。\n                    </p>\n                    <h4 data-nodeid=\"48167\">【数据集二】</h4>\n                    <p data-nodeid=\"48168\">\n                      再运行上述代码，我们对数据集二建模得到的最优参数为 [0.16,\n                      0.10,\n                      -0.03]。利用这组参数，我们可以对数据集二的学习效果进行测试，如下表所示。<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/19/Ciqc1F_lxh2AB8_FAAIMnwsJ144124.png\"\n                        alt=\"图片13.png\"\n                        data-nodeid=\"48599\"\n                      /><br />\n                      我们发现，在数据集二上，模型的预测结果只能是马马虎虎，这体现在两点：\n                    </p>\n                    <ul data-nodeid=\"48169\">\n                      <li data-nodeid=\"48170\">\n                        <p data-nodeid=\"48171\">\n                          4 个样本中，并没有全部正确预测，样本 1 预测错误；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"48172\">\n                        <p data-nodeid=\"48173\">\n                          对于正确预测的 3 个样本而言，预测值都在边界线 0.5\n                          附近，就算是正确预测，也没有压倒性优势。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"48174\">\n                      那么为什么同样的模型，只是换了数据集，效果就千差万别呢？\n                    </p>\n                    <h3 data-nodeid=\"48175\">逻辑回归的不足</h3>\n                    <p data-nodeid=\"48176\">\n                      这是因为，逻辑回归是个线性模型，它只能处理线性问题。\n                    </p>\n                    <p data-nodeid=\"48177\">\n                      例如，对于一个二维平面来说，线性模型就是一条直线。如果数据的分布不支持用一条线来分割的话，逻辑回归就无法收敛，如下图所示。<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lximANu7DAAC6Cmxv9rM944.png\"\n                        alt=\"图片3.png\"\n                        data-nodeid=\"48611\"\n                      /><br />\n                      图中蓝色点是一类，黄色点是一类。现在，我们要用逻辑回归这样的线性模型来进行区分。可见，不论这条线怎么选，都是无法将两类样本进行分割的，这也是逻辑回归模型的缺陷。\n                    </p>\n                    <blockquote data-nodeid=\"48178\">\n                      <p data-nodeid=\"48179\">\n                        要想解决的话，只有用更复杂的模型，例如我们后续的课时中会介绍的决策树、神经网络等模型。\n                      </p>\n                    </blockquote>\n                    <h4 data-nodeid=\"48180\">【逻辑回归与线性回归】</h4>\n                    <p data-nodeid=\"48181\">\n                      在上一讲《18 | AI 入门：利用 3 个公式搭建最简 AI\n                      框架》中，通过“身高预测”，我们从人工智能模型的视角，重新认识了线性回归，那么逻辑回归和线性回归的不同有哪些呢？\n                    </p>\n                    <ul data-nodeid=\"48182\">\n                      <li data-nodeid=\"48183\">\n                        <p data-nodeid=\"48184\">\n                          <strong data-nodeid=\"48622\">从名字上比较</strong>\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"48185\">\n                      线性回归是回归模型，是用一根“线”去回归出输入和输出之间的关系，即用一根线去尽可能把全部样本“串”起来。\n                    </p>\n                    <p data-nodeid=\"48186\">\n                      而逻辑回归虽然名字里有“回归”二字，但其实是一个分类模型，它是希望用一根线去把两波样本尽可能分开。\n                    </p>\n                    <ul data-nodeid=\"48187\">\n                      <li data-nodeid=\"48188\">\n                        <p data-nodeid=\"48189\">\n                          <strong data-nodeid=\"48628\">从表达式上看</strong>\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"48190\">\n                      逻辑回归是在线性回归的基础上加了一个 Sigmoid\n                      函数的映射，在最终的类别判断上，还需要对比一下预测值和 0.5\n                      之间的大小关系。\n                    </p>\n                    <p data-nodeid=\"48191\">\n                      因此，线性回归解决的是回归问题，输出的连续值；而逻辑回归解决的是二分类问题，输出的是“0”或“1”的离散值。\n                    </p>\n                    <ul data-nodeid=\"48192\">\n                      <li data-nodeid=\"48193\">\n                        <p data-nodeid=\"48194\">\n                          <strong data-nodeid=\"48634\">从机理上看</strong>\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"48195\">\n                      逻辑回归增加了 sigmoid 函数，可以让预测结果在 0.5\n                      附近产生更大的变化率，变化率更大，意味着梯度更大。\n                    </p>\n                    <p data-nodeid=\"48196\">\n                      在使用梯度下降法的时候，这样的机理，让模型的预测值会倾向于离开变化率大的地方，而收敛在“0”或“1”附近。这样的模型机理，会让它更适合用于分类问题的建模，具有更好的鲁棒性。\n                    </p>\n                    <h3 data-nodeid=\"48197\">小结</h3>\n                    <p data-nodeid=\"48198\">\n                      <strong data-nodeid=\"48648\">逻辑回归</strong\n                      >是人工智能领域中分类问题的入门级算法。利用 AI\n                      基本框架来看，它的 3 个核心公式分别是<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/30/Ciqc1F_pRgWAY4ynAABjhuo5U0E756.png\"\n                        alt=\"WechatIMG1406.png\"\n                        data-nodeid=\"48645\"\n                      /><br />\n                      逻辑回归是个线性模型，具有计算简单、可解释性强等优势。它的不足是，只能处理线性问题，对于非线性问题则束手无策。\n                    </p>\n                    <p data-nodeid=\"48199\">\n                      最后，我们留一个思考题。试着把本课时中的代码，由随机梯度下降法改写为梯度下降法，再来求解一次参数\n                      <i><strong data-nodeid=\"48656\">w</strong></i>\n                      吧。原则上除了计算量会变大以外，对分类结果是不会产生改变的。不妨亲自试一下。\n                    </p>\n                    <p data-nodeid=\"49254\">\n                      下一课时，我将向你讲解《20 | 决策树：如何对 NP\n                      难复杂问题进行启发式求解？》\n                    </p>\n                    <hr data-nodeid=\"49255\" />\n                    <p data-nodeid=\"49256\" class=\"te-preview-highlight\">\n                      <a\n                        href=\"https://wj.qq.com/s2/7812549/4cd8/\"\n                        data-nodeid=\"49262\"\n                        >课程评价入口，挑选 5 名小伙伴赠送小礼品～</a\n                      >\n                    </p>\n            "}