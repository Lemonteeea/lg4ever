{"title":"20 | 决策树：如何对 NP 难复杂问题进行启发式求解？","context":"\n                    <p data-nodeid=\"52985\" class=\"\">\n                      这一讲，我们学习决策树模型。决策树模型既可以解决分类问题，也可以解决回归问题，经典的决策树算法有\n                      ID3、C4.5，以及 CART 算法。\n                    </p>\n                    <p data-nodeid=\"52986\">\n                      当今主流的人工智能模型都是基于决策树的模型，例如更复杂的梯度提升决策树、随机森林等等。这些模型有着更加复杂、深厚的数学机理，但本质上还是决策树的思想。\n                    </p>\n                    <h3 data-nodeid=\"52987\">决策树及其基本结构</h3>\n                    <p data-nodeid=\"52988\">\n                      决策树算法采用树形结构，使用层层推理来实现最终的分类。与逻辑回归不同，决策树模型很难用一个函数来描述输入向量<i\n                        ><strong data-nodeid=\"53162\">x</strong></i\n                      >和预测类别 y\n                      之间的关系。但是，如果利用一个如下图的树形状图形，就能很轻松描述清楚。\n                    </p>\n                    <p data-nodeid=\"52989\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/59/CgqCHl_q5HuAGijaAACMMrgsWGQ843.png\"\n                        alt=\"图片1.png\"\n                        data-nodeid=\"53165\"\n                      />\n                    </p>\n                    <div data-nodeid=\"52990\">\n                      <p style=\"text-align: center\">决策树</p>\n                    </div>\n                    <p data-nodeid=\"52991\">我们可以发现决策树有以下特点。</p>\n                    <p data-nodeid=\"52992\">\n                      决策树由结点和边组成。最上边的结点称作<strong\n                        data-nodeid=\"53176\"\n                        >根结点</strong\n                      >，最下边的结点称作<strong data-nodeid=\"53177\"\n                        >叶子结点</strong\n                      >。除了叶子结点外，每个结点都根据某个变量及其分界阈值，决定了是向左走或向右走。每个叶子结点代表了某个分类的结果。\n                    </p>\n                    <ul data-nodeid=\"52993\">\n                      <li data-nodeid=\"52994\">\n                        <p data-nodeid=\"52995\">\n                          当使用决策树模型去预测某个样本的归属类别时，需要将这个样本从根结点输入；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"52996\">\n                        <p data-nodeid=\"52997\">\n                          接着就要“按图索骥”，根据决策树中的规则，一步步找到向左走或向右走的路径；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"52998\">\n                        <p data-nodeid=\"52999\">\n                          直到最终，最终到达了某个叶子结点中，并用该叶子结点的类别表示预测结果。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"53000\">\n                      例如，大迷糊的头发长度为 6 厘米、指甲长度为 0.1\n                      厘米，我们要预测大迷糊的性别。从根结点出发，因为大迷糊的头发长度大于\n                      5 厘米，则向左走；又因为大迷糊的指甲长度小于 1\n                      厘米，则向右走。最终抵达叶子结点为男性，这就是预测的结果。\n                    </p>\n                    <h3 data-nodeid=\"53001\">决策树建模的挑战</h3>\n                    <p data-nodeid=\"53002\">\n                      我们曾说过，利用人工智能建模就是建立假设，再去找到假设条件下的最优化参数。对于决策树而言，它的假设就是输入向量<i\n                        ><strong data-nodeid=\"53190\">x</strong></i\n                      >和输出类别 y 之间是一棵树的条件判断关系。\n                    </p>\n                    <p data-nodeid=\"53003\">\n                      这样来看，决策树模型的参数就是每个结点的分裂变量和分裂变量的阈值。决策树建模，就是要找到最优的模型参数，让预测结果尽可能更准。然而，在使用决策树建模时想最优的模型参数是个\n                      NP 难的问题。\n                    </p>\n                    <p data-nodeid=\"53004\">\n                      NP\n                      难问题，指最优参数无法在多项式时间内被计算出来，这很像我们先前所说的指数爆炸。NP\n                      难问题是数学界的一类经典问题，我们这里进行简单介绍。\n                    </p>\n                    <p data-nodeid=\"53005\">\n                      例如，旅行商问题（Travel Saleman Problem or\n                      TSP）就是个典型的 NP\n                      难问题。旅行商问题，是指一个旅行商需要从 A 城市出发，经过\n                      B 城市、C 城市、D 城市等 n 个城市后， 最后返回 A\n                      城市，已知任意两个城市之间的路费 x<sup>ij</sup>。\n                    </p>\n                    <p data-nodeid=\"53006\">\n                      问：这个旅行商以怎样的城市顺序安排旅行，能让自己的路费最少。\n                    </p>\n                    <p data-nodeid=\"53007\">\n                      这个旅行商问题显然就是一个 NP 难问题，这体现在两个方面。\n                    </p>\n                    <ul data-nodeid=\"53008\">\n                      <li data-nodeid=\"53009\">\n                        <p data-nodeid=\"53010\">\n                          第一，任意给出一个行程安排，例如\n                          A-&gt;B-&gt;D-&gt;C-&gt;A，都可以很容易算出旅行路线的总费用；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53011\">\n                        <p data-nodeid=\"53012\">\n                          第二，但是要想找到费用最少的那条路线，最坏情况下，必须检查所有可能的路线，而这里可能的路线是\n                          (n-1)! 个。\n                        </p>\n                      </li>\n                    </ul>\n                    <blockquote data-nodeid=\"53013\">\n                      <p data-nodeid=\"53014\">\n                        例如，3 个城市的路线有\n                        A-&gt;B-&gt;C-&gt;A、A-&gt;C-&gt;B-&gt;A\n                        两种可能，搜索空间决定了时间复杂度，显然复杂度是\n                        O(n!)。这远大于多项式，例如\n                        O(n)、O(n<sup>2</sup>)、O(n<sup>3</sup>) 的时间复杂度。\n                      </p>\n                    </blockquote>\n                    <p data-nodeid=\"53015\">\n                      面对 NP\n                      难问题，常规的解法是降低解的质量，去换取复杂度的降低。简而言之就是，从寻找\n                      NP\n                      难问题的全局最优解，转变为在多项式时间内寻找某个大差不差的次优解。通常，这类算法也被称为启发式的算法。\n                    </p>\n                    <p data-nodeid=\"53016\">\n                      因此，在使用决策树建模时，绝大多数的决策树算法（如 ID3 和\n                      C4.5）所采取的策略都是启发式算法（例如贪心算法）来对空间进行搜索。这样，决策树中的每个结点都是基于当前的局部最优选择进行构造。\n                    </p>\n                    <h3 data-nodeid=\"53017\">ID3 决策树的启发式建模</h3>\n                    <p data-nodeid=\"53018\">\n                      补充完了基本概念后，我们以 ID3\n                      决策树为例，详细探讨一下决策树建模的过程。ID3\n                      决策树的核心思想，是在当前结点，根据信息增益最大的那个特征变量，决定如何构成决策树。\n                    </p>\n                    <blockquote data-nodeid=\"53019\">\n                      <p data-nodeid=\"53020\">\n                        我们在《10 |\n                        信息熵：事件的不确定性如何计算？》曾经学过，利用熵、条件熵来描述事件的不确定性。进一步，可以得到信息增益，来量化某个条件对于事件不确定性降低的多少。\n                      </p>\n                    </blockquote>\n                    <p data-nodeid=\"53021\">\n                      由此可见，ID3\n                      决策树的思路非常简单，就是在所有能降低不确定性的变量中，找到那个降低程度最多的变量作为分裂变量。经过多次重复这个过程，就能得到一棵决策树了。\n                    </p>\n                    <p data-nodeid=\"53022\">\n                      <strong data-nodeid=\"53226\"\n                        >【ID3 决策树建模步骤】</strong\n                      >\n                    </p>\n                    <ul data-nodeid=\"53023\">\n                      <li data-nodeid=\"53024\">\n                        <p data-nodeid=\"53025\">计算出数据集的信息熵。</p>\n                      </li>\n                      <li data-nodeid=\"53026\">\n                        <p data-nodeid=\"53027\">\n                          对于<i><strong data-nodeid=\"53235\">x</strong></i\n                          >向量的每一个维度：\n                        </p>\n                        <ol data-nodeid=\"53028\">\n                          <li data-nodeid=\"53029\">\n                            <p data-nodeid=\"53030\">\n                              以这个维度作为条件，计算条件熵；\n                            </p>\n                          </li>\n                          <li data-nodeid=\"53031\">\n                            <p data-nodeid=\"53032\">\n                              根据数据集的信息熵和条件熵，计算信息增益。\n                            </p>\n                          </li>\n                        </ol>\n                      </li>\n                      <li data-nodeid=\"53033\">\n                        <p data-nodeid=\"53034\">\n                          找到信息增益最大的变量，作为当前的分裂变量，并根据这个分裂变量得到若干个子集。\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53035\">\n                        <p data-nodeid=\"53036\">\n                          对分类过后的每个子集，递归地执行 1～3\n                          步，直到终止条件满足。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"53037\">\n                      <strong data-nodeid=\"53243\"\n                        >【ID3 决策树常见的两个终止条件】</strong\n                      >\n                    </p>\n                    <ul data-nodeid=\"53038\">\n                      <li data-nodeid=\"53039\">\n                        <p data-nodeid=\"53040\">\n                          如果结点中的全部的样本都属于同一类别，则算法停止，并输出类别标签。\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53041\">\n                        <p data-nodeid=\"53042\">\n                          若无法继续对当层节点进行划分（特征用完），将该节点内的最高频的类别标签输出。\n                        </p>\n                      </li>\n                    </ul>\n                    <h4 data-nodeid=\"53043\">论述 ID3 建模的过程 案例 1</h4>\n                    <p data-nodeid=\"53044\">\n                      假设有以下数据集，每一行是一个样本，每一列一个特征变量，最后一列是样本的真实类别。试着去建立\n                      ID3 决策树。<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/9A/Ciqc1F_yin-AM7DrAABm_gQS4Ss061.png\"\n                        alt=\"WechatIMG1513.png\"\n                        data-nodeid=\"53251\"\n                      /><br />\n                      <strong data-nodeid=\"53256\">1.首先，计算信息熵。</strong>\n                    </p>\n                    <p data-nodeid=\"53045\">\n                      数据集中，类别为“1”的样本有 1 个，类别为“0”的样本有 3\n                      个；这样，类别“1”出现的概率就是 1/4，类别“0”出现的概率就是\n                      3/4。\n                    </p>\n                    <p data-nodeid=\"53046\">\n                      根据公式可以知道，信息熵为<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/04/33/CgpVE1_q5PqAQoX-AAAumZoxyjE562.jpg\"\n                        alt=\"图片3.jpg\"\n                        data-nodeid=\"53262\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53047\">\n                      <strong data-nodeid=\"53266\"\n                        >2.接着，对每个变量，计算条件熵及其信息增益</strong\n                      >\n                    </p>\n                    <ul data-nodeid=\"53048\">\n                      <li data-nodeid=\"53049\">\n                        <p data-nodeid=\"53050\">第一个变量</p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"53051\">\n                      第一个变量，即数据集中的第一列。它包含了两个“1”和两个“0”，可见“1”和“0”的概率为1/2。其中在第一个变量为“1”的两个样本中，类别标签分别为“1”和“0”，则信息熵为<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/04/31/Cip5yF_q5ZWAT7kcAAAfYaOrC8U812.jpg\"\n                        alt=\"图片4-1.jpg\"\n                        data-nodeid=\"53272\"\n                      /><br />\n                      在第一个变量为“0”的两个样本中，类别标签都是“0”，则信息熵为<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/59/CgqCHl_q5eiAb068AAAVBDyzwWs832.jpg\"\n                        alt=\"图片4-2.jpg\"\n                        data-nodeid=\"53278\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53052\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/04/33/CgpVE1_q5jiAVL-sAABcTNSFB4w222.jpg\"\n                        alt=\"图片4-3.jpg\"\n                        data-nodeid=\"53281\"\n                      />\n                    </p>\n                    <ul data-nodeid=\"53053\">\n                      <li data-nodeid=\"53054\">\n                        <p data-nodeid=\"53055\">第二个变量</p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"53056\">\n                      同理，可以计算出第二个变量的信息增益为<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/04/31/Cip5yF_q5niATTuzAAAcYoZqEvU417.jpg\"\n                        alt=\"图片5-1.jpg\"\n                        data-nodeid=\"53287\"\n                      />\n                    </p>\n                    <ul data-nodeid=\"53057\">\n                      <li data-nodeid=\"53058\">\n                        <p data-nodeid=\"53059\">第三个变量</p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"53060\">\n                      对于第三个变量，它的值都是 1，也就是说第三个变量出现 1\n                      的概率是 100%。<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/59/CgqCHl_q5rKALD0WAABSP-kjYD0960.jpg\"\n                        alt=\"图片5-2.jpg\"\n                        data-nodeid=\"53293\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53061\">\n                      也就是没有信息增益，等同于是个废话。从数据中也能看出，第三个变量的值对于所有数据样本而言都是一样的，可见它是没有任何区分度的。\n                    </p>\n                    <p data-nodeid=\"53062\">\n                      <strong data-nodeid=\"53298\">3.变量分裂与决策树</strong>\n                    </p>\n                    <p data-nodeid=\"53063\">\n                      基于这个过程，我们选取出信息增益最大的变量为第一个变量，标记为\n                      x<sub>1</sub>（但若信息增益都一样，随机选择一个就可以了）。根据\n                      x<sub>1</sub> 以及 x<sub>1</sub>\n                      可能的取值，可以把决策树暂时建立如下图所示。\n                    </p>\n                    <p data-nodeid=\"53064\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/03/FE/CgpVE1_lymqAcPlvAAA5o6I0_vQ040.png\"\n                        alt=\"Drawing 1.png\"\n                        data-nodeid=\"53314\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53065\">\n                      根据当前的决策树，可以把原数据集切分为两个子集，分别是\n                      D<sub>1</sub> 和 D<sub>2</sub>。\n                    </p>\n                    <p data-nodeid=\"53066\">\n                      <strong data-nodeid=\"53349\">X</strong><sub>1</sub\n                      ><strong data-nodeid=\"53350\">= 0 时，子数据集是 D</strong\n                      ><sub>1</sub><br />\n                      <br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/4E/Ciqc1F_q5tKAO7TMAABDmYKSNK0106.png\"\n                        alt=\"Lark20201229-160955.png\"\n                        data-nodeid=\"53342\"\n                      /><br />\n                      在 D<sub>1</sub>\n                      中，所有样本的类别标签都是“0”，满足了决策树建模的终止条件，则直接输出类别标签“0”，决策树更新为\n                    </p>\n                    <p data-nodeid=\"53067\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lynCAPm2iAAAl-o0va9M449.png\"\n                        alt=\"Drawing 2.png\"\n                        data-nodeid=\"53353\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53068\">\n                      <strong data-nodeid=\"53373\">X</strong><sub>1</sub\n                      ><strong data-nodeid=\"53374\">= 1 时，子数据集是 D</strong\n                      ><sub>2</sub><br />\n                      <br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/4E/Ciqc1F_q5uOAZn6sAAA_T_aOZE4543.png\"\n                        alt=\"Lark20201229-161001.png\"\n                        data-nodeid=\"53372\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53069\">\n                      对于 D<sub>2</sub>\n                      而言，还需要重复计算熵和信息增益。在D<sub>2</sub>中，类别“1”和类别“0”各有一个样本，即出现的概率都是\n                      1/2，因此熵为<br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/04/33/CgpVE1_q5yGAHP4zAAAogkrcJ6k145.jpg\"\n                        alt=\"111.jpg\"\n                        data-nodeid=\"53387\"\n                      />\n                    </p>\n                    <ul data-nodeid=\"53070\">\n                      <li data-nodeid=\"53071\">\n                        <p data-nodeid=\"53072\">\n                          而对于三个变量而言，第一个变量和第三个变量的信息增益都是零。这是因为，两个样本在第一个变量和第三个变量的值是相等的，没有任何信息量；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53073\">\n                        <p data-nodeid=\"53074\">\n                          对于第二个变量而言，条件熵为 H(y|x<sub>2</sub>) =\n                          (1/2)×0 + (1/2)×0 = 0，信息增益为 g(x<sub>2</sub>,y) =\n                          H(p) - H(y|x<sub>2</sub>) = 1。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"53075\">\n                      因此，应该采用第二个变量进行分裂，则有下面的决策树\n                    </p>\n                    <p data-nodeid=\"53076\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lynmAefPmAABCjJ8vz-U348.png\"\n                        alt=\"Drawing 3.png\"\n                        data-nodeid=\"53422\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53077\">\n                      基于这个决策树，如果 x<sub>2</sub> 为 0，则得到子集\n                      D<sub>3</sub>；如果 x<sub>2</sub> 为 1，则得到子集\n                      D<sub>4</sub>。\n                    </p>\n                    <ul data-nodeid=\"53078\">\n                      <li data-nodeid=\"53079\">\n                        <p data-nodeid=\"53080\">\n                          同时，在 D<sub>3</sub> 中，只剩下 [1,0,1,0]\n                          这条样本，直接输出类别标签“0”；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53081\">\n                        <p data-nodeid=\"53082\">\n                          在 D<sub>4</sub> 中，只剩下 [1,1,1,1]\n                          这条样本，直接输出类别标签“1”。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"53083\">\n                      二者都满足了停止条件，这样决策树就建立完成了，结果如下：\n                    </p>\n                    <p data-nodeid=\"53084\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lyoCACXjYAABFsFcIBrk020.png\"\n                        alt=\"Drawing 4.png\"\n                        data-nodeid=\"53461\"\n                      />\n                    </p>\n                    <h4 data-nodeid=\"53085\">论述 ID3 建模的过程 案例 2</h4>\n                    <p data-nodeid=\"53086\">\n                      我们再看一个数据集，如下所示，这也是上一讲中，逻辑回归没有建立出模型的非线性问题的数据集。\n                    </p>\n                    <blockquote data-nodeid=\"53087\">\n                      <p data-nodeid=\"53088\">\n                        其中每一行是一个样本，每一列一个变量，最后一列是样本的类别标签。\n                      </p>\n                    </blockquote>\n                    <p data-nodeid=\"53089\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/4E/Ciqc1F_q6E6ACKyVAABgfXtLDIA680.png\"\n                        alt=\"图片8.png\"\n                        data-nodeid=\"53467\"\n                      /><br />\n                      我们还是可以根据 ID3 决策树的流程来建立模型。\n                    </p>\n                    <p data-nodeid=\"53090\">\n                      <strong data-nodeid=\"53473\">1.首先，计算信息熵</strong>\n                    </p>\n                    <p data-nodeid=\"53091\">\n                      我们发现在数据集中，类别为“1”的样本有两个，类别为“0”的样本也有两个；这样，他们二者出现的概率就都是\n                      1/2。\n                    </p>\n                    <p data-nodeid=\"53092\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/59/CgqCHl_q6ISAWT1LAABnxl4UPqE890.png\"\n                        alt=\"9.png\"\n                        data-nodeid=\"53477\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53093\">\n                      <strong data-nodeid=\"53481\"\n                        >2.接着，对每个变量计算条件熵和信息增益</strong\n                      >\n                    </p>\n                    <p data-nodeid=\"53094\">\n                      对于第一个变量 x1 的值有 1 和 0 两个可能性，出现的概率都是\n                      2/4。\n                    </p>\n                    <p data-nodeid=\"53095\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/04/32/Cip5yF_q6QWASKkQAAGB8GDbqqE903.png\"\n                        alt=\"10.png\"\n                        data-nodeid=\"53485\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53096\">\n                      <strong data-nodeid=\"53489\">3.变量分裂与决策树</strong>\n                    </p>\n                    <p data-nodeid=\"53097\">\n                      当信息增益完全一致的时候，我们随机选择一个作为分裂变量。假设选\n                      x<sub>1</sub>，则根据 x<sub>1</sub>\n                      的不同，可以得到下面的决策树。\n                    </p>\n                    <p data-nodeid=\"53098\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/19/Ciqc1F_lyouAEIX-AAAjk2gZiFs803.png\"\n                        alt=\"Drawing 5.png\"\n                        data-nodeid=\"53501\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53099\">\n                      根据当前的决策树，可以将数据集分割为 D1 和 D2\n                      两部分，并建立决策树。\n                    </p>\n                    <p data-nodeid=\"53100\">\n                      <strong data-nodeid=\"53506\"\n                        >X1 为 0 时，子数据集为 D1</strong\n                      >\n                    </p>\n                    <p data-nodeid=\"53101\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/59/CgqCHl_q6USARgYZAABBkd-FmD0667.png\"\n                        alt=\"Lark20201229-161007.png\"\n                        data-nodeid=\"53509\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53102\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/4E/Ciqc1F_q6VaAIHAuAAA4MCT2o0g658.png\"\n                        alt=\"Lark20201229-161010.png\"\n                        data-nodeid=\"53512\"\n                      />\n                    </p>\n                    <ul data-nodeid=\"53103\">\n                      <li data-nodeid=\"53104\">\n                        <p data-nodeid=\"53105\">\n                          不难发现，在 D<sub>1</sub> 中，第一个变量 x<sub\n                            >1</sub\n                          >\n                          和第三个变量 x<sub>3</sub> 的信息增益都是 0；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53106\"></li>\n                    </ul>\n                    <p data-nodeid=\"53107\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/4F/Ciqc1F_q7EWARz2DAAA7OEkAWV8268.png\"\n                        alt=\"Lark20201229-164322.png\"\n                        data-nodeid=\"53528\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53108\">\n                      可见，需要用 x<sub>2</sub> 对 D<sub>1</sub>\n                      进行拆分，这样就得到了下面的决策树。\n                    </p>\n                    <p data-nodeid=\"53109\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lypOAA_3YAABDonV9NLU584.png\"\n                        alt=\"Drawing 6.png\"\n                        data-nodeid=\"53540\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53110\">\n                      <strong data-nodeid=\"53560\">x</strong><sub>1</sub\n                      ><strong data-nodeid=\"53561\">为 1 时，子数据集是 D</strong\n                      ><sub>2</sub><br />\n                      <br />\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/04/33/CgpVE1_q6ayAOUvvAABBfCLOlU8611.png\"\n                        alt=\"Lark20201229-161012.png\"\n                        data-nodeid=\"53559\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53111\">\n                      对于 D<sub>2</sub>\n                      子集，也用同样的方法，我们直接给出建树的结果如下：\n                    </p>\n                    <p data-nodeid=\"53112\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/03/FE/CgpVE1_lypqAGGukAABa0m8jZLc584.png\"\n                        alt=\"Drawing 7.png\"\n                        data-nodeid=\"53569\"\n                      />\n                    </p>\n                    <p data-nodeid=\"53113\">所剩特征为 0，分裂结束。</p>\n                    <h4 data-nodeid=\"53114\">ID3 决策树的代码实现</h4>\n                    <p data-nodeid=\"53115\">\n                      对于这种像 ID3\n                      这种成型的算法而言，已经有很多被封装好的工具包（如\n                      sklearn）可以直接调用，并不需要自己来自主开发。\n                    </p>\n                    <p data-nodeid=\"53116\">\n                      如果自己来写底层建模的代码，可能需要上百行的代码量。为了给大家展示最核心的部分，我们给出建立\n                      ID3 决策树的伪代码。\n                    </p>\n                    <div class=\"course-code-area\">\n                      <div class=\"copy-btn\">\n                        <div class=\"copy-icon\"></div>\n                        复制代码\n                      </div>\n                      <pre><code data-language=\"python\"><ol><li><div class=\"code-word\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">createTree</span>(<span class=\"hljs-params\">x, y</span>):</span>\n</div></li><li><div class=\"code-word\">\t<span class=\"hljs-keyword\">if</span> 终止条件满足:\n</div></li><li><div class=\"code-word\">\t\t<span class=\"hljs-keyword\">return</span> labels[<span class=\"hljs-number\">0</span>]\n</div></li><li><div class=\"code-word\">\thp = getHp(y)\n</div></li><li><div class=\"code-word\">\txStar = getBestSplitVar(x,y)\n</div></li><li><div class=\"code-word\">\tmodel.save(xStar)\n</div></li><li><div class=\"code-word\">\txSubList = getSubset(xStar,x)\n</div></li><li><div class=\"code-word\">\tySubList = getSubset(xStar,y)\n</div></li><li><div class=\"code-word\">\t<span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> len(xSubList):\n</div></li><li><div class=\"code-word\">\t\tcreateTree(xSubList[i],ySubList[i])\n</div></li><li><div class=\"code-word\">\t<span class=\"hljs-keyword\">return</span> model\n</div></li></ol></code></pre>\n                    </div>\n                    <p data-nodeid=\"53118\">【我们对代码进行走读】</p>\n                    <p data-nodeid=\"53119\">\n                      从开发的角度来看，决策树采用了一种递归式的建模，可见函数主体一定是一个递归结构。这个递归的终止条件，就是\n                      ID3 建树的终止条件。\n                    </p>\n                    <ul data-nodeid=\"53120\">\n                      <li data-nodeid=\"53121\">\n                        <p data-nodeid=\"53122\">\n                          第 3\n                          行，我们在伪代码中，只提及了所有样本一致的情况篇；另一种情况比较少见，可以先不处理。\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53123\">\n                        <p data-nodeid=\"53124\">\n                          第 4 行，我们需要开发个函数 getHp()\n                          来计算当前数据集的熵，计算熵只跟类别标签 y 向量有关。\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53125\">\n                        <p data-nodeid=\"53126\">\n                          第 5\n                          行，我们需要对所有的变量计算条件熵，并比较出谁产生的信息增益最大。此时我们需要开发\n                          getBestSplitVar() 的函数，它同时依赖 x 向量和 y\n                          向量的输入。\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53127\">\n                        <p data-nodeid=\"53128\">\n                          在得到了最优的分裂变量后，我们就完成了一次迭代，可以在第\n                          6 行把它保存在模型中了。\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53129\">\n                        <p data-nodeid=\"53130\">\n                          第 7 行和第 8\n                          行，是基于现有模型，对数据集进行的切分。此时还需要开发一个函数\n                          getSubset()，需要实现的功能是在数据集中基于 xStar\n                          对数据集进行分割，并返回所有子集的 list。\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53131\">\n                        <p data-nodeid=\"53132\">\n                          最后，第 9～10 行，对于每个子集，递归地调用建树的函数\n                          createTree()，再次重复上面的过程。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"53133\">\n                      ID3\n                      决策树建树的代码开发，就是一个递归结构的开发。虽然实际的开发中需要开发多个函数，代码量也是很多的，但从原理来看还是非常简单的。\n                    </p>\n                    <h3 data-nodeid=\"53134\">决策树模型的优势和不足</h3>\n                    <h4 data-nodeid=\"53135\">1.优势</h4>\n                    <p data-nodeid=\"53136\">\n                      从上述结果可以看出，决策树最大的优势，是在原本逻辑回归无法做出准确分类的数据集上，决策树<strong\n                        data-nodeid=\"53590\"\n                        >可以做出正确分类</strong\n                      >。\n                    </p>\n                    <ul data-nodeid=\"53137\">\n                      <li data-nodeid=\"53138\">\n                        <p data-nodeid=\"53139\">\n                          这是因为，逻辑回归方法得到的决策边界总是线性的，它是个只能处理线性问题的线性模型；\n                        </p>\n                      </li>\n                      <li data-nodeid=\"53140\">\n                        <p data-nodeid=\"53141\">\n                          而决策树是按照层次结构的规则生成的，它可以通过增加决策树的层次来模拟更复杂的分类边界，可以用来解决更复杂的非线性问题。\n                        </p>\n                      </li>\n                    </ul>\n                    <p data-nodeid=\"53142\">\n                      同时，在模型的可解释性上，决策树明确给出了预测的依据。要解释决策树如何预测非常简单，从根结点开始，依照所有的特征开始分支，一直到到达叶子节点，找到最终的预测。决策树可以很好地捕捉特征之间的互动和依赖，树形结构也可以很好地可视化。\n                    </p>\n                    <h4 data-nodeid=\"53143\">2.不足</h4>\n                    <p data-nodeid=\"53144\">\n                      ID3\n                      决策树，或者说绝大多数的决策树都不是最优的树结构。这主要是因为建树本来就是个\n                      NP\n                      难问题，导致我们的算法只能采用一些启发式的贪心算法。从一开始，建树的目标就不是去寻找最优解。\n                    </p>\n                    <h3 data-nodeid=\"53145\">小结</h3>\n                    <p data-nodeid=\"53146\">\n                      决策树模型是<strong data-nodeid=\"53602\">浅层模型</strong\n                      >中最优秀、最普适的一类模型。很多提升方法也都是基于决策树演变而来的。\n                    </p>\n                    <p data-nodeid=\"53147\">\n                      在这里我们提到了一个浅层模型的概念，这主要是与深度学习进行的比较。我们知道这几年由于神经网络的兴起，深度学习的概念一下子称为\n                      AI 领域的研究热点。\n                    </p>\n                    <p data-nodeid=\"53148\">\n                      原本，学者们并没有浅层模型的概念。因为深度学习兴起后，产生了很多层次复杂、结构很深的模型；那么与之对应的经典模型，就被人们统称为浅层模型了。\n                    </p>\n                    <p data-nodeid=\"53149\">\n                      然而经过人们的验证会发现，浅层模型中的佼佼者仍然是<strong\n                        data-nodeid=\"53612\"\n                        >树模型</strong\n                      >。而深层模型通过增加了模型的复杂度，换取了更好的效果。关于深层模型，我们会在下一讲《21\n                      |\n                      神经网络与深度学习：计算机是如何理解图像、文本和语音的？》中进行讨论。\n                    </p>\n                    <p data-nodeid=\"53150\">\n                      最后，我们留一个练习题。对于下面的数据集，试着用 ID3\n                      算法建立决策树。\n                    </p>\n                    <p data-nodeid=\"54885\">\n                      <img\n                        src=\"https://s0.lgstatic.com/i/image2/M01/04/32/Cip5yF_q6gyAUHnuAABhPZjS8Mk947.png\"\n                        alt=\"图片13.png\"\n                        data-nodeid=\"54890\"\n                      />\n                    </p>\n                    <hr data-nodeid=\"54886\" />\n                    <p data-nodeid=\"54887\" class=\"te-preview-highlight\">\n                      <a\n                        href=\"https://wj.qq.com/s2/7812549/4cd8/\"\n                        data-nodeid=\"54893\"\n                        >课程评价入口，挑选 5 名小伙伴赠送小礼品～</a\n                      >\n                    </p>\n            "}